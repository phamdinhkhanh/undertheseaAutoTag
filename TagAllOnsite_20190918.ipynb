{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config sentiments:  good_sentiments.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import logging\n",
    "import pickle\n",
    "import warnings\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from underthesea import pos_tag, chunk, sentiment\n",
    "from core_nlp.tokenization import base_tokenizer, dict_models\n",
    "from hyperparameter import Hyper\n",
    "# https://www.dataquest.io/blog/python-json-tutorial\n",
    "\n",
    "# LINK = 'http://10.220.92.51:8983/solr/master_adayroi_PriceRow_default/select?fl=productNameForSearch_text_vi,code_string,brandName_text_vi_mv&fq=(catalogId:%22adayroiProductCatalog%22%20AND%20catalogVersion:%22Online%22)&fq=isValidPrice_boolean:true&fq=isonline_boolean:true&indent=on&q=*:*&wt=json'\n",
    "# LINK = 'http://solrslave01.adayroi.com/solr/master_adayroi_PriceRow_default/select?fl=code_string,categoryNameForSearch_text_vi_mv,categoryName_text_vi_mv,description_text_vi,productNameForSearch_text_vi,brandName_text_vi_mv&fq=(catalogId:%22adayroiProductCatalog%22%20AND%20catalogVersion:%22Online%22)&fq=isValidPrice_boolean:true&fq=isonline_boolean:true&indent=on&q=*:*&wt=json'\n",
    "\n",
    "# warnings.simplefilter('ignore')\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s : %(message)s', level = logging.INFO)\n",
    "# LINK = 'http://10.220.92.51:8983/solr/master_adayroi_PriceRow_default/select?fl=productNameForSearch_text_vi,code_string,brandName_text_vi_mv&fq=(catalogId:%22adayroiProductCatalog%22%20AND%20catalogVersion:%22Online%22)&fq=isValidPrice_boolean:true&fq=isonline_boolean:true&indent=on&q=*:*&wt=json'\n",
    "\n",
    "with open(Hyper.CONFIGFILE, 'rb') as fp:\n",
    "    config = json.loads(fp.read())\n",
    "print('config sentiments: ', config['link_sentiment'])\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Tagging Adayroi onsite production')\n",
    "parser.add_argument('--link', default=config['link'], help='link to clone data from solr')\n",
    "parser.add_argument('--num_product', default=config['num_product'], type=int, help='number of product')\n",
    "parser.add_argument('--link_old', default=config['link_old'], help='link tagging product previous day')\n",
    "parser.add_argument('--link_new', default=config['link_new'], help='link tagging product today')\n",
    "parser.add_argument('--link_output', default=config['link_output'], help='link tagging product with two information productCode and tag_list')\n",
    "parser.add_argument('--link_sentiment', default=config['link_sentiment'], help='link json file include good sentiment code')\n",
    "parser.add_argument('--link_cate', default=config['link_cate'], help='link json file include good sentiment code')\n",
    "args = vars(parser.parse_args([]))\n",
    "\n",
    "\n",
    "class IOObject(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "    def _save_pickle(self, obj):\n",
    "        with open(self.filename, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(obj, fp)\n",
    "\n",
    "    def _open_pickle(self):\n",
    "        with open(self.filename, \"rb\") as fp:   #Unpickling\n",
    "            obj = pickle.load(fp)\n",
    "        return obj\n",
    "\n",
    "    def _save_json(self, obj):\n",
    "        with open(self.filename, \"wb\") as fp:\n",
    "            json.dump(obj, fp)\n",
    "\n",
    "    def _open_json(self):\n",
    "        with open(self.filename, \"rb\") as fp:\n",
    "            obj = json.loads(fp.read())\n",
    "        return obj\n",
    "\n",
    "\n",
    "class TagProductAll(object):\n",
    "    def __init__(self, link):\n",
    "        self.link = link\n",
    "        self.dictionary = self._read_solr()\n",
    "        self.sentiments = IOObject(args['link_sentiment'])._open_json()\n",
    "\n",
    "    # Đọc dữ liệu từ solr\n",
    "    def _read_solr(self):\n",
    "        response1 = urllib.request.urlopen(self.link)\n",
    "        dictionary1 = json.loads(response1.read())\n",
    "        no_product = dictionary1['response']['numFound']\n",
    "        # no_product = args.num_product\n",
    "        logging.info('number of product get: {}'.format(no_product))\n",
    "        self.link = self.link + '&rows=' + str(no_product)\n",
    "        response2 = urllib.request.urlopen(self.link)\n",
    "        dictionary2 = json.loads(response2.read())\n",
    "        self.cate_map_level = self._read_cate(args['link_cate'])\n",
    "        logging.info('completed read link!')\n",
    "        return dictionary2\n",
    "    \n",
    "    # Đọc các mô tả tốt\n",
    "    def _read_sentiments(self):\n",
    "        sentiments = json.loads(args['link_sentiments'])\n",
    "        return sentiments\n",
    "\n",
    "    # Đọc dữ liệu cateName\n",
    "    def _read_cate(self, cate_path=args['link_cate']):\n",
    "        datacate = pd.read_excel(cate_path, sheet_name='displaycate', header=0)\n",
    "        cate_map_level = dict()\n",
    "        for i, row in datacate[['LVL', 'CATENAME']].iterrows():\n",
    "            cate_map_level[row[1]] = row[0]\n",
    "        cate_map_level['Danh mục hiển thị'] = 0\n",
    "        return cate_map_level\n",
    "\n",
    "    # Lọc ra những cate cấp từ 3-4\n",
    "    def _filter_cate(self, texts):\n",
    "        return [text for text in texts if self.cate_map_level[text] in (3, 4)]\n",
    "\n",
    "    # Lấy cate cuối cùng trong danh sách list các cate\n",
    "    def _last_child_cate(self, texts):\n",
    "        return sorted([(self.cate_map_level[text], text) for text in texts], key=lambda x: x[0], reverse=True)[0][1]\n",
    "\n",
    "    # Tạo bảng dữ liệu gồm productCode, productName, tags\n",
    "    def _create_dataset(self):\n",
    "        productName = [p['productNameForSearch_text_vi'] for p in self.dictionary['response']['docs']]\n",
    "        productCode = [p['code_string'] for p in self.dictionary['response']['docs']]\n",
    "        cateName = []\n",
    "        for p in self.dictionary['response']['docs']:\n",
    "            try:\n",
    "                cateName.append(self._last_child_cate(p['categoryName_text_vi_mv']))\n",
    "            except:\n",
    "                cateName.append('')\n",
    "\n",
    "        branchName = []\n",
    "        for p in self.dictionary['response']['docs']:\n",
    "            try:\n",
    "                branchName.append(p['brandName_text_vi_mv'][0])\n",
    "            except:\n",
    "                branchName.append('')\n",
    "\n",
    "        description = []\n",
    "        for p in self.dictionary['response']['docs']:\n",
    "            try:\n",
    "                description.append(p['description_text_vi'])\n",
    "            except:\n",
    "                description.append('')\n",
    "\n",
    "        # drop duplicate productCode\n",
    "        if os.path.exists(args.link_old):\n",
    "            #Đọc file cũ\n",
    "            dataset_old = pd.read_csv(args['link_old'])\n",
    "            #Đọc file mới\n",
    "            dataset_update = pd.DataFrame({'productCode': productCode, 'productName': productName, \n",
    "                                'cateName': cateName, 'branchName': branchName,\n",
    "                                'description': description})\n",
    "            dataset_update.drop_duplicates(subset=['productCode'], inplace=True)\n",
    "            #Danh sách các sản phẩm chung\n",
    "            productCodeCommon = pd.merge(dataset_old, dataset_update, on = 'productCode')['productCode'].tolist()\n",
    "            #Lấy những sản phẩm mới\n",
    "            dataset = dataset_update[~dataset_update['productCode'].isin(productCodeCommon)]\n",
    "            dataset.set_index(np.arange(dataset.shape[0]))\n",
    "            logging.info('Dataset shape: {}'.format(dataset.shape))\n",
    "        else:\n",
    "            dataset = pd.DataFrame({'productCode': productCode, 'productName': productName, \n",
    "                                'cateName': cateName, 'branchName': branchName,\n",
    "                                'description': description})\n",
    "            dataset.drop_duplicates(subset=['productCode'], inplace=True)\n",
    "            dataset.set_index(np.arange(dataset.shape[0]))\n",
    "            logging.info('Dataset shape: {}'.format(dataset.shape))\n",
    "\n",
    "        dataset = self._update_sentiments(dataset)\n",
    "        # Cập nhật tag 1\n",
    "        dataset['tag1'] = [self._gen_tag1(item) for k, item in dataset[['cateName', 'sentiments']].iterrows()]\n",
    "        # Cập nhật tag 2\n",
    "        dataset = self._gen_tag2(dataset)\n",
    "        # cập nhật tag 3\n",
    "        dataset['tag3'] = self._gen_tag3(dataset)\n",
    "        # cập nhật tag 4\n",
    "        dataset['tag4'] = self._gen_tag4(dataset)\n",
    "        # combine các tags\n",
    "        dataset = self._combine_tags(dataset)\n",
    "        logging.info('Completed tag_list!')\n",
    "        return dataset\n",
    "\n",
    "    # Tạo từ điển mô tả các vị trí của từ trong câu\n",
    "    # Ví dụ: 'Trời ơi thật là một ngày đẹp trời. Kết quả trả về bao gồm {'trời': [0, 7], 'ơi':[1], 'thật':[2], 'là':[3], 'một':[4], 'ngày':[5], 'đẹp':[6]}\n",
    "    def _gen_dict_desc(self, text):\n",
    "        desc_dict = defaultdict()\n",
    "        for k, item in enumerate(text.split()):\n",
    "            if item in desc_dict:\n",
    "                desc_dict[item].append(k)\n",
    "            else:\n",
    "                desc_dict[item] = [k]\n",
    "        return desc_dict\n",
    "\n",
    "    # Tính toán khoảng cách nhỏ nhất của 2 cụm vị trí\n",
    "    # Ví dụ: 'trời xanh mây xanh' sau khi tạo từ điển vị trí sẽ là {'trời':[0], 'xanh':[1, 3], 'mây': [2]}\n",
    "    # Đo khoảng cách giữa từ 'xanh' và 'trời' sẽ là 1. Mục đích là để tìm các từ có thể ghép cặp với nhau trong 1 khoảng cách nhất định nếu việc tìm từ đó liền kề nhau là không thể.\n",
    "    # VD: 'trong và sạch' có thể được ghép cặp coi như từ 'trong sạch' bằng cách so khớp khoảng cách.\n",
    "    def _cross_distance(self, first_list, second_list):\n",
    "        return np.min([(y - x) for x in first_list for y in second_list if (y - x) >= 0])\n",
    "\n",
    "    def _check_collocation(self, first, last, desc_dict, min_distance=2):\n",
    "        try:\n",
    "            distance = self._cross_distance(desc_dict[first], desc_dict[last])\n",
    "        except:\n",
    "            return False\n",
    "        if (distance <= min_distance) & (distance >= 0):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # print(_check_collocation('cho', 'tươi', desc_dict = desc_dict))\n",
    "\n",
    "    # Loại bỏ các kí tự đặc biệt bằng khoảng trắng\n",
    "    def _clean_text(self, text, regex=r'[-\\(\\)\\\"#/@;:<>{}\\[\\]`+=~%$&|.*!?,|\\u200b]'):\n",
    "        text = re.sub(regex, r' ', text.lower())\n",
    "        text = re.sub(r'\\s+', r' ', text)\n",
    "        return text\n",
    "\n",
    "    # Kiểm tra một cụm từ có trong mô tả hay không với khoảng cách là 2\n",
    "    def _check_desc(self, search, description, min_distance=2):\n",
    "        words = search.split()\n",
    "        description = self._clean_text(description)\n",
    "        desc_dict = self._gen_dict_desc(description)\n",
    "        if len(words) == 1:\n",
    "            if not words[0] in desc_dict:\n",
    "                return False\n",
    "        else:\n",
    "            for i in range(len(words) - 1):\n",
    "                if not self._check_collocation(words[i], words[i + 1], desc_dict, min_distance):\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    # Lấy ra các comment tốt\n",
    "    def _extract_good_sents(self, text):\n",
    "        return list(set([item for item in self.sentiments if self._check_desc(item.lower(), text.lower())]))\n",
    "\n",
    "\n",
    "    # _check_desc('internet', dataset['description'][0])\n",
    "\n",
    "    # Cập nhật trường comment vào dataset\n",
    "    def _update_sentiments(self, dataset):\n",
    "        sentiments = []\n",
    "        start = datetime.now()\n",
    "        for i, row in dataset[['productName', 'description']].iterrows():\n",
    "            if (i % 1000 == 0) & (i != 0):\n",
    "                end = datetime.now()\n",
    "                print('epochs: {}, time executing: {}'.format(int(i / 1000), end - start))\n",
    "                start = end\n",
    "            sentiments_prod = []\n",
    "            sentiments_des = []\n",
    "            try:\n",
    "                sentiments_prod.extend(self._extract_good_sents(row['productName']))\n",
    "            except:\n",
    "                sentiments_prod.append('')\n",
    "\n",
    "            try:\n",
    "                sentiments_des.extend(self._extract_good_sents(row['description']))\n",
    "            except:\n",
    "                sentiments_des.append('')\n",
    "            sentiments_common = list(set(sentiments_prod).union(set(sentiments_des)))\n",
    "            sentiments.append(sentiments_common)\n",
    "        dataset['sentiments'] = sentiments\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    # I. tag 1: Kết hợp giữa cateName và description =======================================================================================================\n",
    "    # Ví dụ: cateName = 'điện thoại', description = 'màu xanh' ==> 'điện thoại màu xanh'\n",
    "    def _gen_tag1(self, cate_sent):\n",
    "        cate_sent_tags = []\n",
    "        cateName = cate_sent['cateName'].lower()\n",
    "        for sent in cate_sent['sentiments']:\n",
    "            if sent not in cateName:\n",
    "                cate_sent_tags.append(' '.join([cateName, sent]))\n",
    "        return cate_sent_tags\n",
    "\n",
    "\n",
    "    # II. tag 2: Tên sản phẩm chính + description ==========================================================================================================\n",
    "    # Ví dụ: tên sản phẩm chính = 'điện thoại smartphone', description = 'màu xanh' ==> 'điện thoại smartphone màu xanh'\n",
    "    def _gen_tag2(self, dataset):\n",
    "        dataset['productNameClean'] = [self._clean_text(item) for item in dataset['productName']]\n",
    "        dataset['productNameClean'] = [self._clean_text(item) for item in dataset['productName']]\n",
    "        lm_tokenizer = dict_models.LongMatchingTokenizer()\n",
    "        tok_core_nlp = [lm_tokenizer.tokenize(item) for item in dataset['productNameClean']]\n",
    "        dataset['tok_core_nlp'] = tok_core_nlp\n",
    "        # pos tag tên sản phẩm\n",
    "        # mô tả các nhãn post: https://github.com/undertheseanlp/underthesea/wiki/M%C3%B4-t%E1%BA%A3-d%E1%BB%AF-li%E1%BB%87u-b%C3%A0i-to%C3%A1n-POS-Tag\n",
    "        dataset['pos_tag'] = [pos_tag(item) for item in dataset['productNameClean']]\n",
    "        # Tìm danh từ đầu trong tag\n",
    "        dataset['min_pos_tag_N'] = [self._find_position_N(item) for item in dataset['pos_tag']]\n",
    "        # Xác định tên sản phẩm\n",
    "        dataset['main_product_N'] = [self._main_product_N(item['tok_core_nlp'], item['min_pos_tag_N']) for k, item in\n",
    "                                     dataset[['tok_core_nlp', 'min_pos_tag_N']].iterrows()]\n",
    "        # Xác định vị trí tên sản phẩm chính\n",
    "        dataset['len_main_product_N'] = [len(item.split()) for item in dataset['main_product_N']]\n",
    "        # Danh sách các danh từ đặc biệt là những từ có độ dài 1.\n",
    "        self.special_N = set(dataset[dataset['len_main_product_N'] == 1]['main_product_N'])\n",
    "        dataset['min_pos_tag_remove_short_N'] = [self._find_position_remove_short_N(item)\n",
    "                                                 for item in dataset['pos_tag']]\n",
    "        dataset['main_product_remove_short_N'] = [self._main_product_N(item['tok_core_nlp'], item['min_pos_tag_remove_short_N'])\n",
    "                                                  for k, item in dataset[['tok_core_nlp', 'min_pos_tag_remove_short_N']].iterrows()]\n",
    "        dataset['tag2'] = [self._gen_tag_main_product_desc(item)\n",
    "                           for k, item in dataset[['main_product_remove_short_N', 'sentiments']].iterrows()]\n",
    "        return dataset\n",
    "\n",
    "    # Tìm vị trí đầu tiên mà từ bắt đầu là danh từ trong thuật toán pos_tag\n",
    "    def _find_position_N(self, pos_tag):\n",
    "        pos = 0\n",
    "        for item in enumerate(pos_tag):\n",
    "            # nếu là danh từ đầu tiên thì trả về pos\n",
    "            if ('N' in item[1]):\n",
    "                return pos\n",
    "            else:\n",
    "                pos += len(str(item[0]).split())\n",
    "        return pos\n",
    "\n",
    "\n",
    "    # Xuất phát từ danh từ trọng tâm (tên sản phẩm) thường đứng đầu nên ta sẽ ghép các từ từ tok_core_nlp tại vị trí từ min_pos_tag_N đổ về trước\n",
    "    def _main_product_N(self, tokenization, min_pos_tag_N=0):\n",
    "        pos = 0\n",
    "        for i, item in enumerate(tokenization):\n",
    "            if pos > min_pos_tag_N:\n",
    "                return re.sub('_', ' ', ' '.join(tokenization[:i]))\n",
    "            else:\n",
    "                pos += len(item.split('_'))\n",
    "        return re.sub('_', ' ', ' '.join(tokenization[:1]))\n",
    "\n",
    "    # Loại bỏ những main product chỉ có độ dài là 1.\n",
    "    def _find_position_remove_short_N(self, pos_tag):\n",
    "        pos = 0\n",
    "        for i, item in enumerate(pos_tag):\n",
    "            # Nếu từ đầu tiên là danh từ và nằm trong danh sách special_N chọn từ tiếp theo (tiếp tục vòng lặp)\n",
    "            if (i == 0) & ('N' in item[1]) & (item[0] in self.special_N):\n",
    "                next\n",
    "            # Nếu từ tiếp theo là danh từ trả về kết quả\n",
    "            elif ('N' in item[1]):\n",
    "                pos += len(item[0].split())\n",
    "                return pos\n",
    "            # Nếu không thì cộng vào độ dài của các từ liền trước\n",
    "            else:\n",
    "                pos += len(item[0].split())\n",
    "        return pos\n",
    "\n",
    "    # Tạo tag 2: Kết hợp giữa main_product_remove_short_N và description\n",
    "    def _gen_tag_main_product_desc(self, main_product_sent):\n",
    "        main_product_sent_tags = []\n",
    "        for sent in main_product_sent['sentiments']:\n",
    "            if sent not in main_product_sent['main_product_remove_short_N']:\n",
    "                main_product_sent_tags.append(' '.join([main_product_sent['main_product_remove_short_N'], sent]))\n",
    "        return main_product_sent_tags\n",
    "\n",
    "    # III. tag 3: kết hợp giữa cateName và BranchName ==========================================================================================================\n",
    "    # Ví dụ: cateName = 'điện thoại', branchName = 'samsung'\n",
    "    # cateName + branchName = 'điện thoại samsung'\n",
    "    def _gen_tag3(self, dataset):\n",
    "        tag3 = []\n",
    "        for k, (cateName, branch_name) in dataset[['cateName', 'branchName']].iterrows():\n",
    "            branch_name = str(branch_name).lower()\n",
    "            cateName = cateName.lower()\n",
    "            if branch_name in cateName:\n",
    "                tag3.append(cateName)\n",
    "            elif cateName == '':\n",
    "                tag3.append('')\n",
    "            else:\n",
    "                # Loại bỏ branch name nếu như đã xuất hiện trong cateName.\n",
    "                tag = ' '.join([cateName, branch_name]).split()\n",
    "                tag = ' '.join(sorted(set(tag), key=tag.index))\n",
    "                tag3.append(tag)\n",
    "        return tag3\n",
    "\n",
    "    # IV. tag 4: tên sản phẩm chính + branchName =================================================================================================\n",
    "    # Ví dụ: tên sản phẩm chính = 'điện thoại smartphone' + branchName = 'samsung' ==> 'điện thoại smartphone samsung'\n",
    "    def _gen_tag4(self, dataset):\n",
    "        tag4 = []\n",
    "        for k, (main_product, branch_name) in dataset[['main_product_remove_short_N', 'branchName']].iterrows():\n",
    "            branch_name = str(branch_name).lower()\n",
    "            if branch_name in main_product:\n",
    "                tag4.append(main_product)\n",
    "            else:\n",
    "                # remove duplicate word\n",
    "                tag = ' '.join([main_product, branch_name]).split()\n",
    "                tag = ' '.join(sorted(set(tag), key=tag.index))\n",
    "                tag4.append(tag)\n",
    "        return tag4\n",
    "\n",
    "    # Kết hợp toàn bộ các tag1 - tag4\n",
    "    def _combine_tags(self, dataset):\n",
    "        keyword_suggestions = []\n",
    "        for k, row in dataset[['tag2', 'tag3', 'tag4']].iterrows():\n",
    "            row_kw = row['tag2']\n",
    "            row_kw += [row['tag3']]\n",
    "            row_kw += [row['tag4']]\n",
    "            keyword_suggestions.append(row_kw)\n",
    "        keyword_suggestions = [set(item) for item in keyword_suggestions]\n",
    "        dataset['tags'] = [[item for item in tags if item != ''] for tags in keyword_suggestions]\n",
    "        return dataset\n",
    "\n",
    "\n",
    "tagProduct = TagProductAll(args['link'])\n",
    "result = tagProduct._create_dataset()\n",
    "result.to_csv(args['link_new'])\n",
    "result = result[['productCode', 'productName','tags']]\n",
    "if os.path.exists(args.link_old):\n",
    "    result_old = pd.read_csv(args['link_old'], header = 0, index_col = 0)\n",
    "    result = pd.concat([result_old, result], axis = 0)\n",
    "    result.index = np.arange(result.shape[0])\n",
    "    result.to_csv(args['link_output'])\n",
    "    result.to_csv(args['link_old'])\n",
    "else:\n",
    "    result.to_csv(args['link_old'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bền', 'đẹp', 'chắc chắn', 'tốt', 'ngon']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(config['link_sentiment'], 'rb') as fp:\n",
    "    good_sentiments = json.loads(fp.read())\n",
    "    \n",
    "good_sentiments[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
